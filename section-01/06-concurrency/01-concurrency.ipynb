{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concurrency\n",
    "\n",
    "A **concurrent** program is one consisting of multiple threads executing \"at the same\" and independently.\n",
    "\n",
    "A **thread** is an independent line of execution within a process.\n",
    "\n",
    "The OS the first concurrent program. It needs to be a multithreaded program and so the support must be built into the OS itself.\n",
    "\n",
    "A thread has its own set of registers (its context!) on the CPU.\n",
    "\n",
    "A thread has its own stack.\n",
    "\n",
    "A thread has its own instruction pointer (or program counter).\n",
    "\n",
    "Within a process, the rest of the address space is shared amongst all its threads:\n",
    "- heap\n",
    "- code\n",
    "- static global block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<img src=\"images/01-AS.png\" width=\"500\">\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having multiple stacks is not a problem because we can space them out far enough to guarantee that they can't grow into each (for example by having the free space between them be larger than the systems RAM)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why write concurrent programs?\n",
    "\n",
    "If have CPU intensive programs where we can break up the work into chunks and execute it independently, we can split the workload into many threads and run the whole thing faster.\n",
    "\n",
    "If we IO intensive programs, we can split the IO parts into threads while computations continue in the background.\n",
    "\n",
    "Web-servers are implemented as multi-threaded programs. When a request (google query) comes into a web server, a new thread is created to handle that request while the server continues listening for more requests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges for Concurrent Programs\n",
    "\n",
    "Concurrent programs can be non-deterministic because the order that threads are executed is determined by the CPU scheduler.\n",
    "\n",
    "This is especially problematic when multiple threads are accessing shared data.\n",
    "\n",
    "If a thread is interrupted in the middle of accessing shared data, that data can get corrupted.\n",
    "\n",
    "If our threads can get interrupted in the middle of some critical instruction  or set of instructions, then the data that they are modifying may no longer be what we expect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terminology\n",
    "\n",
    "Any set of instructions which should not be interrupted if we want our program to be correct is refered to as a **critical section**.\n",
    "\n",
    "We want our critical sections to be **atomic**.\n",
    "\n",
    "A set of instructions are **atomic** if they can not be interrupted.\n",
    "\n",
    "If multiple threads end up in the same critical section at the same time, we have a **race condition**.\n",
    "\n",
    "To avoid race conditions, to ensure that critical sections are atomic, we use **locks**.\n",
    "\n",
    "We will explore how to use and build locks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use a lock\n",
    "\n",
    "A **lock** is a variable (technically a struct) that only one thread can hold at a time.\n",
    "\n",
    "If we put a lock around a critical section, we ensure that only one thread can access this critical section at a time.\n",
    "\n",
    "We make that section mutually exclusive.\n",
    "\n",
    "To use a lock, a program instantiates it, then attempts to hold it before entering some critical section, and finally upon leaving the critical section releases it.\n",
    "\n",
    "```c\n",
    "lock_t mutex; // create the loct, mutex for mutal exclusion\n",
    "// ...\n",
    "lock(&mutex); // attempt to grab the lock\n",
    "sharedVariable++; // critical section\n",
    "unlock(&mutex); // release the lock\n",
    "```\n",
    "\n",
    "If thread A holds the lock when thread B attempts to grab it, then Thread B must wait until A releases the lock before it can get it and continue execution.\n",
    "\n",
    "A program can create multiple locks. If we have multiple critical sections, we want to have a lock for each critical section so that different threads can execute different critical sections simultaneously.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Lock\n",
    "\n",
    "## Goals\n",
    "\n",
    "**Correctness**: Our lock should correctly ensure mutualy exclusion for critical sections.\n",
    "\n",
    "**Fairness**: If we have multiple threads all vying for the same lock, eventually each thread should be able to get the lock. We don't want to starve our threads.\n",
    "\n",
    "**Performance**: Minimize overhead. Within a single the overhead should be minimal. Over the whole system, we want to minimize the overall time spent waiting for locks."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
